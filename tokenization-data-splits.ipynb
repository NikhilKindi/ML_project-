{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14113264,"sourceType":"datasetVersion","datasetId":8990273}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\n\n# ======================================\n# SETTINGS\n# ======================================\nABC_DIR = \"/kaggle/input/abc-notation-cleaned/ABC-4_clean\"\nOUTPUT_BIN = \"/kaggle/working/dataset.bin\"\nMETA_FILE = \"/kaggle/working/meta.pkl\"\n\n# ======================================\n# 1. FIRST PASS — Build vocab (streaming)\n# ======================================\ndef build_vocab(folder):\n    files = glob(os.path.join(folder, \"*.abc\"))\n    print(\"Total ABC files found:\", len(files))\n\n    chars = set()\n\n    for f in tqdm(files, desc=\"Building vocab\"):\n        with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n            for line in fp:\n                # Remove T: lines if you want cleaner vocab\n                if line.startswith(\"T:\"):\n                    continue\n                chars.update(line)\n\n    unique_chars = sorted(list(chars))\n    vocab_size = len(unique_chars)\n\n    print(\"\\n===== FINAL VOCAB =====\")\n    print(\"Vocab size:\", vocab_size)\n    print(unique_chars)\n    print(\"=======================\\n\")\n\n    stoi = {ch: i for i, ch in enumerate(unique_chars)}\n    itos = {i: ch for i, ch in enumerate(unique_chars)}\n\n    return stoi, itos, vocab_size\n\n\n# ======================================\n# 2. SECOND PASS — Encode and write to .bin (streaming)\n# ======================================\ndef encode(folder, stoi, output_path):\n    files = glob(os.path.join(folder, \"*.abc\"))\n    total_tokens = 0\n\n    with open(output_path, \"wb\") as out:\n        for f in tqdm(files, desc=\"Encoding to BIN\"):\n            with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n                for line in fp:\n                    # Skip titles\n                    if line.startswith(\"T:\"):\n                        continue\n                    # Convert chars to ints\n                    token_ids = [stoi[c] for c in line]\n                    arr = np.array(token_ids, dtype=np.uint16)\n                    arr.tofile(out)\n                    total_tokens += len(arr)\n\n    return total_tokens\n\n\n# ======================================\n# MAIN\n# ======================================\nif __name__ == \"__main__\":\n\n    print(\"\\nSTEP 1 — BUILD VOCAB\")\n    stoi, itos, vocab_size = build_vocab(ABC_DIR)\n\n    print(\"\\nSTEP 2 — ENCODE DATASET\")\n    total_tokens = encode(ABC_DIR, stoi, OUTPUT_BIN)\n\n    print(\"\\nSaved dataset.bin →\", OUTPUT_BIN)\n    print(\"Total tokens:\", total_tokens)\n\n    # Save meta info\n    meta = {\n        \"vocab_size\": vocab_size,\n        \"itos\": itos,\n        \"stoi\": stoi,\n    }\n    with open(META_FILE, \"wb\") as f:\n        pickle.dump(meta, f)\n\n    print(\"\\nSaved meta.pkl →\", META_FILE)\n    print(\"\\nTokenization COMPLETED SUCCESSFULLY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T01:09:28.226358Z","iopub.execute_input":"2025-12-13T01:09:28.226942Z","iopub.status.idle":"2025-12-13T01:59:48.488687Z","shell.execute_reply.started":"2025-12-13T01:09:28.226916Z","shell.execute_reply":"2025-12-13T01:59:48.487916Z"}},"outputs":[{"name":"stdout","text":"\nSTEP 1 — BUILD VOCAB\nTotal ABC files found: 175609\n","output_type":"stream"},{"name":"stderr","text":"Building vocab: 100%|██████████| 175609/175609 [02:42<00:00, 1082.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n===== FINAL VOCAB =====\nVocab size: 99\n['\\t', '\\n', '\\x0b', '\\x0c', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n=======================\n\n\nSTEP 2 — ENCODE DATASET\n","output_type":"stream"},{"name":"stderr","text":"Encoding to BIN: 100%|██████████| 175609/175609 [47:34<00:00, 61.52it/s] ","output_type":"stream"},{"name":"stdout","text":"\nSaved dataset.bin → /kaggle/working/dataset.bin\nTotal tokens: 3844030892\n\nSaved meta.pkl → /kaggle/working/meta.pkl\n\nTokenization COMPLETED SUCCESSFULLY\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pickle\n\n# ======================================\n# INPUT / OUTPUT PATHS\n# ======================================\n\nINPUT_BIN = \"/kaggle/working/dataset.bin\"    # your big token file\nMETA_FILE = \"/kaggle/working/meta.pkl\"\n\nTRAIN_BIN = \"/kaggle/working/train.bin\"\nVAL_BIN   = \"/kaggle/working/val.bin\"\nTEST_BIN  = \"/kaggle/working/test.bin\"\n\n# ======================================\n# 1. LOAD VOCAB + META\n# ======================================\n\nwith open(META_FILE, \"rb\") as f:\n    meta = pickle.load(f)\n\nvocab_size = meta[\"vocab_size\"]\nprint(\"Vocab size =\", vocab_size)\n\n# ======================================\n# 2. MEMORY MAP DATASET (DO NOT LOAD)\n# ======================================\nprint(\"Memory-mapping dataset...\")\n\ndata = np.memmap(INPUT_BIN, dtype=np.uint16, mode='r')\nN = len(data)\n\nprint(\"Total tokens found:\", N)\n\n# Safety check\n#assert N > 100_000_000, \"Dataset too small. Need 100M+ tokens.\"\n\n# ======================================\n# 3. DEFINE SPLIT SIZES\n# ======================================\ntrain_size = int(N * 0.98)\nval_size   = int(N * 0.01)\ntest_size  = N - train_size - val_size  # remaining 1%\n\nprint(\"\\nSplit sizes:\")\nprint(\"Train:\", train_size)\nprint(\"Val:  \", val_size)\nprint(\"Test: \", test_size)\n\nassert train_size >= 100_000_000, \"Training split must have 100M+ tokens.\"\n\n# ======================================\n# 4. WRITE SPLITS USING MEMMAP\n# ======================================\n\ndef save_split(out_path, start, end):\n    print(f\"Writing {out_path} ...\")\n    sub = np.memmap(out_path, dtype=np.uint16, mode='w+', shape=(end - start,))\n    sub[:] = data[start:end]\n    del sub   # flush to disk\n\n# --- Train split ---\nsave_split(TRAIN_BIN, 0, train_size)\n\n# --- Val split ---\nsave_split(VAL_BIN, train_size, train_size + val_size)\n\n# --- Test split ---\nsave_split(TEST_BIN, train_size + val_size, N)\n\nprint(\"\\nSplit complete!\")\nprint(\"Saved:\")\nprint(\" →\", TRAIN_BIN)\nprint(\" →\", VAL_BIN)\nprint(\" →\", TEST_BIN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T01:59:48.489470Z","iopub.execute_input":"2025-12-13T01:59:48.489659Z","iopub.status.idle":"2025-12-13T02:00:24.721797Z","shell.execute_reply.started":"2025-12-13T01:59:48.489645Z","shell.execute_reply":"2025-12-13T02:00:24.721065Z"}},"outputs":[{"name":"stdout","text":"Vocab size = 99\nMemory-mapping dataset...\nTotal tokens found: 3844030892\n\nSplit sizes:\nTrain: 3767150274\nVal:   38440308\nTest:  38440310\nWriting /kaggle/working/train.bin ...\nWriting /kaggle/working/val.bin ...\nWriting /kaggle/working/test.bin ...\n\nSplit complete!\nSaved:\n → /kaggle/working/train.bin\n → /kaggle/working/val.bin\n → /kaggle/working/test.bin\n","output_type":"stream"}],"execution_count":2}]}