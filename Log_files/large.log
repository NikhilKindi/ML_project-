Overriding config with config/abc_large.py:
# config/abc_large.py
# Large GPT (~50M parameters, 24 layers)

# -------------------
# Output & logging
# -------------------
out_dir = '/content/drive/MyDrive/nanoGPT_runs/out-abc-large'
eval_interval = 1000
eval_iters = 200
log_interval = 200
always_save_checkpoint = True

# -------------------
# Dataset
# -------------------
dataset = 'abc'

# -------------------
# Batch / streaming
# -------------------
block_size = 256
batch_size = 8
gradient_accumulation_steps = 16
# Effective tokens / iteration = 32768

# -------------------
# Model (~50M params)
# -------------------
n_layer = 24
n_head  = 16
n_embd  = 512
dropout = 0.1

# -------------------
# Training (1 epoch = 200M tokens)
# -------------------
learning_rate = 3e-4
max_iters = 6104
lr_decay_iters = max_iters
warmup_iters = 500
min_lr = 1e-5

weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# -------------------
# System
# -------------------
device = 'cuda'
dtype = 'float16'
compile = False

tokens per iteration will be: 32,768
found vocab_size = 99 (inside data/abc/meta.pkl)
Initializing a new model from scratch
number of parameters: 75.57M
num decayed parameter tensors: 98, with 75,679,232 parameters
num non-decayed parameter tensors: 49, with 25,088 parameters
using fused AdamW: True
step 0: train loss 4.6494, val loss 4.6456
iter 0: loss 4.6189, time 8142.25ms, mfu -100.00%
iter 200: loss 1.3814, time 725.68ms, mfu 7.11%
iter 400: loss 0.9271, time 713.24ms, mfu 7.12%
iter 600: loss 0.8645, time 709.39ms, mfu 7.14%
iter 800: loss 0.5757, time 725.90ms, mfu 7.13%
step 1000: train loss 0.5519, val loss 0.5652
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 1000: loss 0.4072, time 9334.81ms, mfu 6.48%
iter 1200: loss 0.5700, time 723.61ms, mfu 6.54%
iter 1400: loss 0.5206, time 706.88ms, mfu 6.62%
iter 1600: loss 0.5236, time 705.60ms, mfu 6.69%
iter 1800: loss 0.4494, time 703.22ms, mfu 6.75%
step 2000: train loss 0.4720, val loss 0.4650
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 2000: loss 0.7366, time 9631.18ms, mfu 6.13%
iter 2200: loss 0.4732, time 715.46ms, mfu 6.24%
iter 2400: loss 0.4808, time 709.61ms, mfu 6.34%
iter 2600: loss 0.5237, time 693.17ms, mfu 6.45%
iter 2800: loss 0.3908, time 726.72ms, mfu 6.52%
step 3000: train loss 0.4497, val loss 0.4412
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 3000: loss 0.4160, time 12090.87ms, mfu 5.91%
iter 3200: loss 0.6309, time 726.27ms, mfu 6.03%
iter 3400: loss 0.6520, time 712.66ms, mfu 6.15%
iter 3600: loss 0.3624, time 694.99ms, mfu 6.28%
iter 3800: loss 0.5666, time 709.52ms, mfu 6.37%
step 4000: train loss 0.4374, val loss 0.4278
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 4000: loss 0.4263, time 9705.73ms, mfu 5.79%
iter 4200: loss 0.3573, time 707.10ms, mfu 5.94%
iter 4400: loss 0.5121, time 718.40ms, mfu 6.06%
iter 4600: loss 0.4103, time 711.42ms, mfu 6.18%
iter 4800: loss 0.5058, time 702.11ms, mfu 6.30%
step 5000: train loss 0.4050, val loss 0.4043
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 5000: loss 0.6936, time 9586.87ms, mfu 5.72%
iter 5200: loss 0.5549, time 724.21ms, mfu 5.86%
iter 5400: loss 0.5564, time 708.23ms, mfu 6.01%
iter 5600: loss 0.3853, time 706.29ms, mfu 6.14%
iter 5800: loss 0.3966, time 721.50ms, mfu 6.24%
step 6000: train loss 0.4224, val loss 0.4073
saving checkpoint to /content/drive/MyDrive/nanoGPT_runs/out-abc-large
iter 6000: loss 0.4499, time 11903.33ms, mfu 5.66%
