{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14137319,"sourceType":"datasetVersion","datasetId":9008900},{"sourceId":14137324,"sourceType":"datasetVersion","datasetId":9008905},{"sourceId":14137800,"sourceType":"datasetVersion","datasetId":9009288}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/karpathy/nanoGPT.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T11:33:17.855027Z","iopub.execute_input":"2025-12-13T11:33:17.855344Z","iopub.status.idle":"2025-12-13T11:33:18.651513Z","shell.execute_reply.started":"2025-12-13T11:33:17.855325Z","shell.execute_reply":"2025-12-13T11:33:18.650529Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'nanoGPT'...\nremote: Enumerating objects: 689, done.\u001b[K\nremote: Total 689 (delta 0), reused 0 (delta 0), pack-reused 689 (from 1)\u001b[K\nReceiving objects: 100% (689/689), 975.24 KiB | 19.50 MiB/s, done.\nResolving deltas: 100% (382/382), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\nmkdir -p data/abc\n\nln -s /kaggle/input/training-data/train.bin data/abc/train.bin\nln -s /kaggle/input/validation-set/val.bin  data/abc/val.bin\nln -s /kaggle/input/meta-data/meta.pkl      data/abc/meta.pkl\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Small Transformer** ","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\ncat << 'EOF' > config/abc_small.py\n# config/abc_small.py\n# Small GPT (~5–6M parameters)\n\n# -------------------\n# Output & logging\n# -------------------\nout_dir = '/kaggle/working/nanoGPT_runs/out-abc-small'\neval_interval = 1000\neval_iters = 200\nlog_interval = 200\nalways_save_checkpoint = True\n\n# -------------------\n# Dataset\n# -------------------\ndataset = 'abc'\n\n# -------------------\n# Batch / streaming\n# -------------------\nblock_size = 256\nbatch_size = 8\ngradient_accumulation_steps = 16\n# Effective tokens / iteration = 32768\n\n# -------------------\n# Model (Small)\n# -------------------\nn_layer = 8\nn_head  = 8\nn_embd  = 256\ndropout = 0.1\n\n# -------------------\n# Training (1 epoch = 200M tokens)\n# -------------------\nlearning_rate = 3e-4\nmax_iters = 6104\nlr_decay_iters = max_iters\nwarmup_iters = 500\nmin_lr = 1e-5\n\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n# -------------------\n# System\n# -------------------\ndevice = 'cuda'\ndtype = 'float16'\ncompile = False\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:13:30.447231Z","iopub.execute_input":"2025-12-13T08:13:30.447699Z","iopub.status.idle":"2025-12-13T08:13:30.457942Z","shell.execute_reply.started":"2025-12-13T08:13:30.447682Z","shell.execute_reply":"2025-12-13T08:13:30.457294Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\npython -u train.py config/abc_small.py | tee small.log\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:14:52.371286Z","iopub.execute_input":"2025-12-13T08:14:52.372110Z","iopub.status.idle":"2025-12-13T08:56:25.851515Z","shell.execute_reply.started":"2025-12-13T08:14:52.372079Z","shell.execute_reply":"2025-12-13T08:56:25.850917Z"}},"outputs":[{"name":"stdout","text":"Overriding config with config/abc_small.py:\n# config/abc_small.py\n# Small GPT (~5–6M parameters)\n\n# -------------------\n# Output & logging\n# -------------------\nout_dir = '/kaggle/working/nanoGPT_runs/out-abc-small'\neval_interval = 1000\neval_iters = 200\nlog_interval = 200\nalways_save_checkpoint = True\n\n# -------------------\n# Dataset\n# -------------------\ndataset = 'abc'\n\n# -------------------\n# Batch / streaming\n# -------------------\nblock_size = 256\nbatch_size = 8\ngradient_accumulation_steps = 16\n# Effective tokens / iteration = 32768\n\n# -------------------\n# Model (Small)\n# -------------------\nn_layer = 8\nn_head  = 8\nn_embd  = 256\ndropout = 0.1\n\n# -------------------\n# Training (1 epoch = 200M tokens)\n# -------------------\nlearning_rate = 3e-4\nmax_iters = 6104\nlr_decay_iters = max_iters\nwarmup_iters = 500\nmin_lr = 1e-5\n\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n# -------------------\n# System\n# -------------------\ndevice = 'cuda'\ndtype = 'float16'\ncompile = False\n\ntokens per iteration will be: 32,768\nfound vocab_size = 99 (inside data/abc/meta.pkl)\nInitializing a new model from scratch\nnumber of parameters: 6.32M\nnum decayed parameter tensors: 34, with 6,382,336 parameters\nnum non-decayed parameter tensors: 17, with 4,352 parameters\nusing fused AdamW: True\nstep 0: train loss 4.5635, val loss 4.5643\niter 0: loss 4.5433, time 4856.32ms, mfu -100.00%\niter 200: loss 1.3242, time 400.70ms, mfu 1.16%\niter 400: loss 1.0242, time 401.11ms, mfu 1.16%\niter 600: loss 0.9407, time 407.41ms, mfu 1.16%\niter 800: loss 0.8312, time 401.79ms, mfu 1.16%\nstep 1000: train loss 0.7211, val loss 0.6871\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 1000: loss 0.6539, time 4828.47ms, mfu 1.05%\niter 1200: loss 0.5892, time 401.27ms, mfu 1.06%\niter 1400: loss 0.6311, time 401.96ms, mfu 1.07%\niter 1600: loss 0.7725, time 401.72ms, mfu 1.08%\niter 1800: loss 0.4573, time 402.00ms, mfu 1.09%\nstep 2000: train loss 0.5686, val loss 0.5427\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 2000: loss 0.5652, time 4525.79ms, mfu 0.99%\niter 2200: loss 0.6590, time 402.52ms, mfu 1.01%\niter 2400: loss 0.6974, time 402.03ms, mfu 1.02%\niter 2600: loss 0.5561, time 402.34ms, mfu 1.03%\niter 2800: loss 0.6819, time 401.92ms, mfu 1.05%\nstep 3000: train loss 0.4963, val loss 0.5126\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 3000: loss 0.7024, time 4140.33ms, mfu 0.95%\niter 3200: loss 0.5356, time 401.89ms, mfu 0.97%\niter 3400: loss 0.4432, time 402.04ms, mfu 0.99%\niter 3600: loss 0.6568, time 402.44ms, mfu 1.01%\niter 3800: loss 0.4962, time 402.31ms, mfu 1.02%\nstep 4000: train loss 0.4793, val loss 0.4851\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 4000: loss 0.3433, time 4068.42ms, mfu 0.93%\niter 4200: loss 0.4652, time 401.84ms, mfu 0.95%\niter 4400: loss 0.4416, time 401.65ms, mfu 0.97%\niter 4600: loss 0.4496, time 401.91ms, mfu 0.99%\niter 4800: loss 0.3269, time 401.41ms, mfu 1.01%\nstep 5000: train loss 0.4801, val loss 0.4613\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 5000: loss 0.4221, time 4027.43ms, mfu 0.92%\niter 5200: loss 0.4019, time 401.27ms, mfu 0.94%\niter 5400: loss 0.2955, time 401.27ms, mfu 0.96%\niter 5600: loss 0.4099, time 401.14ms, mfu 0.98%\niter 5800: loss 0.4146, time 400.93ms, mfu 1.00%\nstep 6000: train loss 0.4821, val loss 0.4823\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-small\niter 6000: loss 0.4729, time 3974.07ms, mfu 0.91%\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**Medium Transformer**","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\ncat << 'EOF' > config/abc_medium.py\n# config/abc_medium.py\n# Medium GPT (~20M parameters)\n\n# -------------------\n# Output & logging\n# -------------------\nout_dir = '/kaggle/working/nanoGPT_runs/out-abc-medium'\neval_interval = 1000\neval_iters = 200\nlog_interval = 200\nalways_save_checkpoint = True\n\n# -------------------\n# Dataset\n# -------------------\ndataset = 'abc'\n\n# -------------------\n# Batch / streaming\n# -------------------\nblock_size = 256\nbatch_size = 8\ngradient_accumulation_steps = 16\n# Effective tokens / iteration = 32768\n\n# -------------------\n# Model (Medium)\n# -------------------\nn_layer = 12\nn_head  = 6\nn_embd  = 384\ndropout = 0.1\n\n# -------------------\n# Training (1 epoch = 200M tokens)\n# -------------------\nlearning_rate = 3e-4\nmax_iters = 6104\nlr_decay_iters = max_iters\nwarmup_iters = 500\nmin_lr = 1e-5\n\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n# -------------------\n# System\n# -------------------\ndevice = 'cuda'\ndtype = 'float16'\ncompile = False\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:58:29.585940Z","iopub.execute_input":"2025-12-13T08:58:29.586194Z","iopub.status.idle":"2025-12-13T08:58:29.596251Z","shell.execute_reply.started":"2025-12-13T08:58:29.586175Z","shell.execute_reply":"2025-12-13T08:58:29.595632Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\npython -u train.py config/abc_medium.py | tee medium.log\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:58:33.886542Z","iopub.execute_input":"2025-12-13T08:58:33.887126Z","iopub.status.idle":"2025-12-13T10:29:15.251002Z","shell.execute_reply.started":"2025-12-13T08:58:33.887104Z","shell.execute_reply":"2025-12-13T10:29:15.250238Z"}},"outputs":[{"name":"stdout","text":"Overriding config with config/abc_medium.py:\n# config/abc_medium.py\n# Medium GPT (~20M parameters)\n\n# -------------------\n# Output & logging\n# -------------------\nout_dir = '/kaggle/working/nanoGPT_runs/out-abc-medium'\neval_interval = 1000\neval_iters = 200\nlog_interval = 200\nalways_save_checkpoint = True\n\n# -------------------\n# Dataset\n# -------------------\ndataset = 'abc'\n\n# -------------------\n# Batch / streaming\n# -------------------\nblock_size = 256\nbatch_size = 8\ngradient_accumulation_steps = 16\n# Effective tokens / iteration = 32768\n\n# -------------------\n# Model (Medium)\n# -------------------\nn_layer = 12\nn_head  = 6\nn_embd  = 384\ndropout = 0.1\n\n# -------------------\n# Training (1 epoch = 200M tokens)\n# -------------------\nlearning_rate = 3e-4\nmax_iters = 6104\nlr_decay_iters = max_iters\nwarmup_iters = 500\nmin_lr = 1e-5\n\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n# -------------------\n# System\n# -------------------\ndevice = 'cuda'\ndtype = 'float16'\ncompile = False\n\ntokens per iteration will be: 32,768\nfound vocab_size = 99 (inside data/abc/meta.pkl)\nInitializing a new model from scratch\nnumber of parameters: 21.28M\nnum decayed parameter tensors: 50, with 21,369,984 parameters\nnum non-decayed parameter tensors: 25, with 9,600 parameters\nusing fused AdamW: True\nstep 0: train loss 4.6931, val loss 4.6870\niter 0: loss 4.6529, time 8950.79ms, mfu -100.00%\niter 200: loss 1.4664, time 878.88ms, mfu 1.70%\niter 400: loss 1.2047, time 880.92ms, mfu 1.69%\niter 600: loss 0.7212, time 880.22ms, mfu 1.69%\niter 800: loss 0.7319, time 881.15ms, mfu 1.69%\nstep 1000: train loss 0.6073, val loss 0.5959\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 1000: loss 0.6960, time 8787.98ms, mfu 1.54%\niter 1200: loss 0.6640, time 881.71ms, mfu 1.56%\niter 1400: loss 0.3503, time 881.15ms, mfu 1.57%\niter 1600: loss 0.7810, time 880.87ms, mfu 1.58%\niter 1800: loss 0.5861, time 881.03ms, mfu 1.59%\nstep 2000: train loss 0.5105, val loss 0.4956\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 2000: loss 0.6429, time 8971.31ms, mfu 1.45%\niter 2200: loss 0.8892, time 880.99ms, mfu 1.47%\niter 2400: loss 0.4750, time 881.10ms, mfu 1.50%\niter 2600: loss 0.4909, time 881.05ms, mfu 1.52%\niter 2800: loss 0.4650, time 881.95ms, mfu 1.53%\nstep 3000: train loss 0.4647, val loss 0.4682\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 3000: loss 0.5089, time 9080.44ms, mfu 1.40%\niter 3200: loss 0.5859, time 880.97ms, mfu 1.43%\niter 3400: loss 0.4704, time 880.94ms, mfu 1.45%\niter 3600: loss 0.4414, time 880.82ms, mfu 1.48%\niter 3800: loss 0.5281, time 881.05ms, mfu 1.50%\nstep 4000: train loss 0.4480, val loss 0.4399\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 4000: loss 0.5091, time 9125.60ms, mfu 1.36%\niter 4200: loss 0.5079, time 880.80ms, mfu 1.40%\niter 4400: loss 0.4926, time 881.09ms, mfu 1.43%\niter 4600: loss 0.4364, time 881.37ms, mfu 1.45%\niter 4800: loss 0.3220, time 881.52ms, mfu 1.48%\nstep 5000: train loss 0.4340, val loss 0.4325\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 5000: loss 0.5381, time 9006.94ms, mfu 1.35%\niter 5200: loss 0.4509, time 881.04ms, mfu 1.38%\niter 5400: loss 0.4236, time 881.12ms, mfu 1.41%\niter 5600: loss 0.4013, time 880.81ms, mfu 1.44%\niter 5800: loss 0.3980, time 879.79ms, mfu 1.46%\nstep 6000: train loss 0.4368, val loss 0.4262\nsaving checkpoint to /kaggle/working/nanoGPT_runs/out-abc-medium\niter 6000: loss 0.3818, time 8976.96ms, mfu 1.33%\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"RNN Training- Different Sizes","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\ncat << 'EOF' > lstm_scaling.py\nimport os, time, math, json, pickle\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# =========================\n# CONFIG (MATCH TRANSFORMER)\n# =========================\nDATA_DIR = os.path.join(\"data\", \"abc\")\nOUT_DIR  = \"out-lstm\"\nDEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nBATCH_SIZE = 8\nBLOCK_SIZE = 256\nGRAD_ACCUM = 16                 # 8*256*16 = 32768 tokens/step\nTOKEN_BUDGET = 200_000_000      # <<< 200M TOKENS (FINAL)\n\nLR = 3e-4\nMIN_LR = 1e-5\nWARMUP_ITERS = 500\nWEIGHT_DECAY = 1e-1\nBETAS = (0.9, 0.95)\nGRAD_CLIP = 1.0\n\nEVAL_INTERVAL = 2000\nEVAL_ITERS = 200\nLOG_INTERVAL = 500\n\nDTYPE = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n\n# =========================\n# DATA\n# =========================\ntrain_data = np.memmap(os.path.join(DATA_DIR, \"train.bin\"), dtype=np.uint16, mode=\"r\")\nval_data   = np.memmap(os.path.join(DATA_DIR, \"val.bin\"),   dtype=np.uint16, mode=\"r\")\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    ix = torch.randint(0, len(data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n    x = torch.stack([torch.from_numpy((data[i:i+BLOCK_SIZE]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+BLOCK_SIZE]).astype(np.int64)) for i in ix])\n    return x.to(DEVICE), y.to(DEVICE)\n\n@torch.no_grad()\ndef estimate_loss(model):\n    model.eval()\n    losses = []\n    for _ in range(EVAL_ITERS):\n        x, y = get_batch(\"val\")\n        with torch.autocast(device_type=\"cuda\", dtype=DTYPE, enabled=(DEVICE==\"cuda\")):\n            _, loss = model(x, y)\n        losses.append(loss.item())\n    model.train()\n    return float(np.mean(losses))\n\n# =========================\n# MODEL\n# =========================\nclass LSTMLM(nn.Module):\n    def __init__(self, vocab_size, n_embd, n_layer):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, n_embd)\n        self.lstm = nn.LSTM(n_embd, n_embd, n_layer, batch_first=True)\n        self.ln = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        x = self.emb(idx)\n        y, _ = self.lstm(x)\n        y = self.ln(y)\n        logits = self.head(y)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\ndef count_params(m):\n    return sum(p.numel() for p in m.parameters())\n\ndef lr_schedule(it, max_iters):\n    if it < WARMUP_ITERS:\n        return LR * it / WARMUP_ITERS\n    if it >= max_iters:\n        return MIN_LR\n    decay_ratio = (it - WARMUP_ITERS) / (max_iters - WARMUP_ITERS)\n    return MIN_LR + 0.5 * (LR - MIN_LR) * (1 + math.cos(math.pi * decay_ratio))\n\n# =========================\n# TRAIN ONE MODEL\n# =========================\ndef train_model(name, n_layer, n_embd):\n    with open(os.path.join(DATA_DIR, \"meta.pkl\"), \"rb\") as f:\n        vocab_size = pickle.load(f)[\"vocab_size\"]\n\n    os.makedirs(OUT_DIR, exist_ok=True)\n    log_path = os.path.join(OUT_DIR, f\"{name}.jsonl\")\n\n    model = LSTMLM(vocab_size, n_embd, n_layer).to(DEVICE)\n    n_params = count_params(model)\n\n    tokens_per_iter = BATCH_SIZE * BLOCK_SIZE * GRAD_ACCUM\n    max_iters = math.ceil(TOKEN_BUDGET / tokens_per_iter)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=LR, betas=BETAS, weight_decay=WEIGHT_DECAY)\n\n    print(f\"\\n{name}: {n_params/1e6:.2f}M params | iters={max_iters}\")\n    start = time.time()\n\n    with open(log_path, \"w\") as f:\n        f.write(json.dumps({\n            \"name\": name,\n            \"params\": n_params,\n            \"token_budget\": TOKEN_BUDGET,\n            \"tokens_per_iter\": tokens_per_iter,\n            \"max_iters\": max_iters\n        }) + \"\\n\")\n\n    for it in range(1, max_iters + 1):\n        lr = lr_schedule(it, max_iters)\n        for pg in opt.param_groups:\n            pg[\"lr\"] = lr\n\n        opt.zero_grad(set_to_none=True)\n        total_loss = 0.0\n\n        for _ in range(GRAD_ACCUM):\n            xb, yb = get_batch(\"train\")\n            with torch.autocast(device_type=\"cuda\", dtype=DTYPE, enabled=(DEVICE==\"cuda\")):\n                _, loss = model(xb, yb)\n                loss = loss / GRAD_ACCUM\n            loss.backward()\n            total_loss += loss.item()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        opt.step()\n\n        if it % LOG_INTERVAL == 0:\n            print(f\"{name} | iter {it}/{max_iters}\")\n\n        if it % EVAL_INTERVAL == 0 or it == max_iters:\n            val_loss = estimate_loss(model)\n            with open(log_path, \"a\") as f:\n                f.write(json.dumps({\"iter\": it, \"val_loss\": val_loss}) + \"\\n\")\n            print(f\"{name} | VAL @ {it}: {val_loss:.4f}\")\n\n    print(f\"{name} DONE in {(time.time()-start)/3600:.2f}h\")\n\n# =========================\n# RUN 4 SIZES\n# =========================\nif __name__ == \"__main__\":\n    runs = [\n        (\"lstm_tiny\",   2,  256),\n        (\"lstm_small\",  3,  448),\n        (\"lstm_medium\", 3,  896),\n        (\"lstm_large\",  4, 1280),\n    ]\n    for r in runs:\n        train_model(*r)\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:06.056747Z","iopub.execute_input":"2025-12-13T12:22:06.057021Z","iopub.status.idle":"2025-12-13T12:22:06.072915Z","shell.execute_reply.started":"2025-12-13T12:22:06.056979Z","shell.execute_reply":"2025-12-13T12:22:06.072389Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/nanoGPT\npython -u lstm_scaling.py | tee lstm_scaling.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:22:11.474509Z","iopub.execute_input":"2025-12-13T12:22:11.474768Z","iopub.status.idle":"2025-12-13T23:22:47.789090Z","shell.execute_reply.started":"2025-12-13T12:22:11.474747Z","shell.execute_reply":"2025-12-13T23:22:47.788319Z"}},"outputs":[{"name":"stdout","text":"\nlstm_tiny: 1.10M params | iters=6104\nlstm_tiny | iter 500/6104\nlstm_tiny | iter 1000/6104\nlstm_tiny | iter 1500/6104\nlstm_tiny | iter 2000/6104\nlstm_tiny | VAL @ 2000: 0.7188\nlstm_tiny | iter 2500/6104\nlstm_tiny | iter 3000/6104\nlstm_tiny | iter 3500/6104\nlstm_tiny | iter 4000/6104\nlstm_tiny | VAL @ 4000: 0.6507\nlstm_tiny | iter 4500/6104\nlstm_tiny | iter 5000/6104\nlstm_tiny | iter 5500/6104\nlstm_tiny | iter 6000/6104\nlstm_tiny | VAL @ 6000: 0.6413\nlstm_tiny | VAL @ 6104: 0.6497\nlstm_tiny DONE in 0.72h\n\nlstm_small: 4.92M params | iters=6104\nlstm_small | iter 500/6104\nlstm_small | iter 1000/6104\nlstm_small | iter 1500/6104\nlstm_small | iter 2000/6104\nlstm_small | VAL @ 2000: 0.6475\nlstm_small | iter 2500/6104\nlstm_small | iter 3000/6104\nlstm_small | iter 3500/6104\nlstm_small | iter 4000/6104\nlstm_small | VAL @ 4000: 0.5688\nlstm_small | iter 4500/6104\nlstm_small | iter 5000/6104\nlstm_small | iter 5500/6104\nlstm_small | iter 6000/6104\nlstm_small | VAL @ 6000: 0.5453\nlstm_small | VAL @ 6104: 0.5662\nlstm_small DONE in 1.49h\n\nlstm_medium: 19.47M params | iters=6104\nlstm_medium | iter 500/6104\nlstm_medium | iter 1000/6104\nlstm_medium | iter 1500/6104\nlstm_medium | iter 2000/6104\nlstm_medium | VAL @ 2000: 0.5747\nlstm_medium | iter 2500/6104\nlstm_medium | iter 3000/6104\nlstm_medium | iter 3500/6104\nlstm_medium | iter 4000/6104\nlstm_medium | VAL @ 4000: 0.5312\nlstm_medium | iter 4500/6104\nlstm_medium | iter 5000/6104\nlstm_medium | iter 5500/6104\nlstm_medium | iter 6000/6104\nlstm_medium | VAL @ 6000: 0.5139\nlstm_medium | VAL @ 6104: 0.5067\nlstm_medium DONE in 2.83h\n\nlstm_large: 52.73M params | iters=6104\nlstm_large | iter 500/6104\nlstm_large | iter 1000/6104\nlstm_large | iter 1500/6104\nlstm_large | iter 2000/6104\nlstm_large | VAL @ 2000: 0.5493\nlstm_large | iter 2500/6104\nlstm_large | iter 3000/6104\nlstm_large | iter 3500/6104\nlstm_large | iter 4000/6104\nlstm_large | VAL @ 4000: 0.5134\nlstm_large | iter 4500/6104\nlstm_large | iter 5000/6104\nlstm_large | iter 5500/6104\nlstm_large | iter 6000/6104\nlstm_large | VAL @ 6000: 0.4942\nlstm_large | VAL @ 6104: 0.4991\nlstm_large DONE in 5.97h\n","output_type":"stream"}],"execution_count":23}]}